{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c54141",
   "metadata": {},
   "source": [
    "## Spam Email Classifier with KNN using TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17102e",
   "metadata": {},
   "source": [
    "1.   Assignment must be implemented in Python 3 only.\n",
    "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for evaluation metrics, data visualization (matplotlib etc.).\n",
    "3.   You will be evaluated not just on the overall performance of the model and also on the experimentation with hyper parameters, data prepossessing techniques etc.\n",
    "4.   The report file must be a well documented jupyter notebook, explaining the experiments you have performed, evaluation metrics and corresponding code. The code must run and be able to reproduce the accuracies, figures/graphs etc.\n",
    "5.   For all the questions, you must create a train-validation data split and test the hyperparameter tuning on the validation set. Your jupyter notebook must reflect the same.\n",
    "6.   Strict plagiarism checking will be done. An F will be awarded for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34a310",
   "metadata": {},
   "source": [
    "**Task: Given an email, classify it as spam or ham**\n",
    "\n",
    "Given input text file (\"emails.txt\") containing 5572 email messages, with each row having its corresponding label (spam/ham) attached to it.\n",
    "\n",
    "This task also requires basic pre-processing of text (like removing stopwords, stemming/lemmatizing, replacing email_address with 'email-tag', etc..).\n",
    "\n",
    "You are required to find the tf-idf scores for the given data and use them to perform KNN using Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c87696",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5a1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "# from contractions import contractions_dict\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "# 3 type of stemmer in order of accuracy snowball > porter > lancaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef4dff",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d925c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('emails.txt', sep='\\t', header=None, names=[\"type\", \"msg\"])\n",
    "\n",
    "df = df[0:3000]\n",
    "\n",
    "# df\n",
    "# display(df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ef5ba",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1733d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"preprocess the given mail\"\"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    message = message.lower()\n",
    "    \n",
    "    # Removing Tags\n",
    "    message = re.sub('<[^<]+?>','', message)\n",
    "\n",
    "    # Removing Links\n",
    "    message = re.sub(r'http\\S+', '', message)\n",
    "    message = re.sub(r'www.\\S+', '', message)\n",
    "    \n",
    "    # Contraction\n",
    "\n",
    "    # Removing Numbers and Punctuations\n",
    "    message = ''.join(c for c in message if not (c.isdigit() or c in punctuation))\n",
    "\n",
    "    # Word Tokenize\n",
    "    message_tokenized = nltk.word_tokenize(message)\n",
    "\n",
    "    # Removing Stop Words\n",
    "    message_tokenized = [word for word in message_tokenized if not (word in stopword or len(word) < 3)]\n",
    "\n",
    "    # Lemmatizing\n",
    "    # reason why not using lemmatizing here:\n",
    "    # using both lemmatizing and stemming will not improve much\n",
    "    # lemmatizing is slow\n",
    "\n",
    "    # Stemming\n",
    "    message_tokenized = [snowball_stemmer.stem(word) for word in message_tokenized]\n",
    "    # print (stemmed_word)\n",
    "    \n",
    "    return message_tokenized\n",
    "    \n",
    "df[\"msg\"] = df[\"msg\"].apply(preprocess)\n",
    "# df        \n",
    "# display(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76767a7",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75e6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.sample(frac=1, replace=False), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "# train, validate, test = np.split(df.sample(frac=1, replace=False), [int(.6*len(df)), int(.8*len(df))])\n",
    "train_data, validate_data, test_data = np.split(df, [int(.6*len(df)), int(.8*len(df))])\n",
    "# print(train_data.shape)\n",
    "# # print(train)\n",
    "# print(validate_data.shape)\n",
    "# # print(validate)\n",
    "# print(test_data.shape)\n",
    "# # print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22e7b",
   "metadata": {},
   "source": [
    "### TF-IDF Utility Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417cbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_words = dict()\n",
    "\n",
    "# def countUniqueWords(message):\n",
    "#     \"\"\"Function to count unique words\"\"\"\n",
    "#     for word  in message:\n",
    "#         if word not in unique_words:\n",
    "#             unique_words[word] = 0\n",
    "#             # unique_words_count[word] = 0\n",
    "#         # unique_words_count[word] += 1\n",
    "\n",
    "# def TF(message):\n",
    "#     \"\"\"Function to calculate TF of data\"\"\"\n",
    "#     tf_message = unique_words.copy()\n",
    "#     count_words = len(message)\n",
    "#     for word in message:\n",
    "#         tf_message[word] += 1\n",
    "#         # count_words += 1\n",
    "#     if count_words != 0:\n",
    "#         tf_message.update((x, y/count_words) for x, y in tf_message.items())\n",
    "#     return tf_message\n",
    "\n",
    "# def IDF():\n",
    "#     \"\"\"Function to calculate IDF of words\"\"\"\n",
    "#     idf_words = unique_words.copy()\n",
    "#     no_of_sentence = len(train)\n",
    "#     for word in idf_words:\n",
    "#         word_in_sentence = 0\n",
    "#         for i in range(0, no_of_sentence):\n",
    "#             # print(train.iloc[i].loc[\"msg\"][word])\n",
    "#             if train.at[i, \"msg\"][word] > 0:\n",
    "#                 word_in_sentence += 1\n",
    "#         idf_words[word] = np.log(no_of_sentence / word_in_sentence)\n",
    "#     return idf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d50993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"msg\"].apply(countUniqueWords)\n",
    "# # print(unique_words)\n",
    "# # print(len(unique_words))\n",
    "# # print(unique_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3924056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lll= list()\n",
    "# lll.append(train[\"msg\"].apply(TF))\n",
    "# # for d in train[\"msg\"]:\n",
    "# #     print(d)\n",
    "# # print(train)\n",
    "# # print(lll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ae40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_words = IDF()\n",
    "# # idf_words = IDF()\n",
    "# # print(idf_words)\n",
    "\n",
    "# # for i in range(0,len(train)):\n",
    "# #     print(train.iloc[i].loc[\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9ffbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = dict()\n",
    "for message in train_data[\"msg\"]:\n",
    "    for word in message:\n",
    "        if word not in unique_words:\n",
    "            unique_words[word] = 0.0\n",
    "\n",
    "# IDF of train\n",
    "train_idf = unique_words.copy()\n",
    "no_of_sentence = len(train_data)\n",
    "for word in train_idf:\n",
    "    word_in_sentence = 0\n",
    "    for message in train_data[\"msg\"]:\n",
    "        if word in message:\n",
    "            word_in_sentence += 1\n",
    "    train_idf[word] = np.log(no_of_sentence / word_in_sentence - 1)\n",
    "# print(train_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f1ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_TF(data_frame):\n",
    "    tf = list()\n",
    "    for message in data_frame[\"msg\"]:\n",
    "        len_message = len(message)\n",
    "        counter_dict = Counter(message)\n",
    "        message_tf = unique_words.copy()\n",
    "        for word, word_count in counter_dict.items():\n",
    "            message_tf[word] = word_count / len_message\n",
    "        tf.append(message_tf)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7fb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(data_frame):\n",
    "    tf = cal_TF(data_frame)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c095610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = get_tfidf(train_data)\n",
    "# validate_tfidf = get_tfidf(validate_data)\n",
    "# test_tfidf = get_tfidf(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12cd525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_tfidf[0])\n",
    "pd.DataFrame(train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6eb76b",
   "metadata": {},
   "source": [
    "### Train your KNN model (reuse previously iplemented model built from scratch) and test on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22baf6b2",
   "metadata": {},
   "source": [
    "***1. Experiment with different distance measures [Euclidean distance, Manhattan distance, Hamming Distance] and compare with the Cosine Similarity distance results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1bb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf6f3b1",
   "metadata": {},
   "source": [
    "***2. Explain which distance measure works best and why? Explore the distance measures and weigh their pro and cons in different application settings.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae57a01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45a99c76",
   "metadata": {},
   "source": [
    "***3. Report Mean Squared Error(MSE), Mean-Absolute-Error(MAE), R-squared (R2) score in a tabular form***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9668814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4dde8d3",
   "metadata": {},
   "source": [
    "***4. Choose different K values (k=1,3,5,7,11,17,23,28) and experiment. Plot a graph showing R2 score vs k.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0fd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15000352",
   "metadata": {},
   "source": [
    "### Train and test Sklearn's KNN classifier model on your data (use metric which gave best results on your experimentation with built-from-scratch model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aab7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d22aa47",
   "metadata": {},
   "source": [
    "***Compare both the models result.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a5274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64987575",
   "metadata": {},
   "source": [
    "***What is the time complexity of training using KNN classifier?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770c106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fad1f345",
   "metadata": {},
   "source": [
    "***What is the time complexity while testing? Is KNN a linear classifier or can it learn any boundary?***"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0daaa324",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
